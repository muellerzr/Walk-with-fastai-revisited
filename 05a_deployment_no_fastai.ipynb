{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a4e1c-a5b6-4093-aac3-b42f305a613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tvf\n",
    "import torchvision.transforms as tvtfms\n",
    "import operator as op\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "# For type hinting later on\n",
    "import collections\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18e089-4375-4aa3-8f62-fac603f458f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_model(\"vit_tiny_patch16_224\", pretrained=False, num_classes=0, in_chans=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b7142-69e6-4182-85f6-893a51dee085",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = nn.Sequential(\n",
    "    nn.BatchNorm1d(192),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(192, 512, bias=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 37, bias=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8c22b-eca7-40d4-97bd-fc28421dad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(net, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6868944-1498-422e-994e-6fa15fd2136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"models/MyModel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbd956-0d28-45ee-aa5d-33eff1a32b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba897ed8-1df7-4240-834c-a39d23ddb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5f1e0-5605-49ff-8405-d929f102ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(state.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5099e3-8379-4946-85a3-aeb672489e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weight(name, parameter, state_dict):\n",
    "    \"\"\"\n",
    "    Takes in a layer `name`, model `parameter`, and `state_dict`\n",
    "    and loads the weights from `state_dict` into `parameter`\n",
    "    if it exists.\n",
    "    \"\"\"\n",
    "   \n",
    "    if name[0] == \"0\":\n",
    "        name = name[:2] + \"model.\" + name[2:]\n",
    "    if name in state_dict.keys():\n",
    "        input_parameter = state_dict[name]\n",
    "        if input_parameter.shape == parameter.shape:\n",
    "            parameter.copy_(input_parameter)\n",
    "        else:\n",
    "            print(f'Shape mismatch at layer: {name}, skipping')\n",
    "    else:\n",
    "        print(f'{name} is not in the state_dict, skipping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec54c5-4d27-43ac-aaf0-00eae7b18fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weights(input_model:nn.Module, input_weights:collections.OrderedDict, application_function:callable):\n",
    "    \"\"\"\n",
    "    Takes an input state_dict and applies those weights to the `input_model`, potentially \n",
    "    with a modifier function.\n",
    "    \n",
    "    Args:\n",
    "        input_model (`nn.Module`):\n",
    "            The model that weights should be applied to\n",
    "        input_weights (`collections.OrderedDict`):\n",
    "            A dictionary of weights, the trained model's `state_dict()`\n",
    "        application_function (`callable`):\n",
    "            A function that takes in one parameter and layer name from `input_model`\n",
    "            and the `input_weights`. Should apply the weights from the state dict into `input_model`.\n",
    "    \"\"\"\n",
    "    model_dict = input_model.state_dict()\n",
    "    for name, parameter in model_dict.items():\n",
    "        application_function(name, parameter, input_weights)\n",
    "    input_model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c884993-cb52-412e-9cad-8d1b9dfff70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_weights(model, state, copy_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5620c43-fb68-42e1-becd-96c74046a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.data import PILImage\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import get_image_files\n",
    "import fastai.vision.augment as fastai_aug\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ea83f-8b9b-45c1-baf8-ee43adde1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'\n",
    "fname = get_image_files(path)[0]\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31184db2-06a4-46da-beba-864345c6dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pil = Image.open(fname)\n",
    "im_fastai = PILImage.create(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603843e3-5448-47c9-a937-87e0a3bfe886",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.array(im_pil) == np.array(im_fastai)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe49581-c4e6-4d73-8b74-e7875962f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_fastai = fastai_aug.RandomResizedCrop((460, 460))\n",
    "crop_torch = tvtfms.CenterCrop((460,460))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ae350-afb6-41f0-bad3-1ddbf52bb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.array(crop_fastai(im_fastai, split_idx=1)) == np.array(crop_torch(im_pil))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ce70b-d412-47b3-8f8b-d9c1af4c02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image:typing.Union[Image.Image, torch.tensor], size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and crops it `size` unless one \n",
    "    dimension is larger than the actual image. Padding \n",
    "    must be performed afterwards if so.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform cropping on\n",
    "        size (`tuple` of integers):\n",
    "            A size to crop to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    top = max(top, 0)\n",
    "    left = max(left, 0)\n",
    "    \n",
    "    height = min(top + size[0], image.shape[-1])\n",
    "    width = min(left + size[1], image.shape[-2])\n",
    "    return image.crop((top, left, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55abb74-5e23-4930-a9f3-b260e6b99aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(image:typing.Union[Image.Image, torch.tensor], size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and pads it to `size` with\n",
    "    zeros.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform padding on\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    pad_top = max(-top, 0)\n",
    "    pad_left = max(-left, 0)\n",
    "    \n",
    "    height, width = (\n",
    "        max(size[1] - image.shape[-1] + top, 0), \n",
    "        max(size[0] - image.shape[-2] + left, 0)\n",
    "    )\n",
    "    return tvf.pad(\n",
    "        image, \n",
    "        [pad_top, pad_left, height, width], \n",
    "        padding_mode=\"constant\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccbc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resized_crop_pad(\n",
    "    image: Union[Image.Image, torch.tensor],\n",
    "    size: Tuple[int, int],\n",
    "    extra_crop_ratio: float = 0.14,\n",
    ") -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image`, resize it according to the\n",
    "    `extra_crop_ratio`, and then crops and pads\n",
    "    it to `size`.\n",
    "\n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform padding on\n",
    "        size (`tuple` of integers):\n",
    "            A size to crop and pad to, should be in the form\n",
    "            of (width, height)\n",
    "        extra_crop_ratio (float):\n",
    "            The ratio of size at the edge cropped out. Default 0.14\n",
    "    \"\"\"\n",
    "\n",
    "    maximum_space = max(size[0], size[1])\n",
    "    extra_space = maximum_space * extra_crop_ratio\n",
    "    extra_space = math.ceil(extra_space / 8) * 8\n",
    "    extended_size = (size[0] + extra_space, size[1] + extra_space)\n",
    "    resized_image = image.resize(extended_size, resample=Image.Resampling.BILINEAR)\n",
    "\n",
    "    if extended_size != size:\n",
    "        resized_image = pad(crop(resized_image, size), size)\n",
    "\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f08caa-723e-4167-8756-ec79b9a7698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (460,460)\n",
    "tfmd_img = resized_crop_pad(im_pil, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d384de6-bb93-4296-a664-a5d5a530dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(tfmd_img) == crop_fastai(im_fastai, split_idx=1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b23700-4b94-4775-b5f9-cee632f39b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_crop(\n",
    "    batch:torch.tensor, \n",
    "    size:typing.Tuple[int,int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Crops each image in `batch` to a particular `size`.\n",
    "    \n",
    "    Args:\n",
    "        batch (array of `torch.Tensor`):\n",
    "            A batch of images, should be of shape `NxCxWxH`\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        A batch of cropped images\n",
    "    \"\"\"\n",
    "   \n",
    "    affine_matrix = torch.eye(3, device=batch.device).float()\n",
    "    affine_matrix = affine_matrix.unsqueeze(0)\n",
    "    affine_matrix = affine_matrix.expand(batch.size(0), 3, 3)\n",
    "    affine_matrix = affine_matrix.contiguous()[:,:2]\n",
    "    \n",
    "    coords = F.affine_grid(\n",
    "        affine_matrix, batch.shape[:2] + size, align_corners=True\n",
    "    )\n",
    "    \n",
    "    top_range, bottom_range = coords.min(), coords.max()\n",
    "    zoom = 1/(bottom_range - top_range).item()*2\n",
    "    \n",
    "    resizing_limit = min(\n",
    "        batch.shape[-2]/coords.shape[-2],\n",
    "        batch.shape[-1]/coords.shape[-1]\n",
    "    )/2\n",
    "    \n",
    "    if resizing_limit > 1 and resizing_limit > zoom:\n",
    "        batch = F.interpolate(\n",
    "            batch, \n",
    "            scale_factor=1/resizing_limit, \n",
    "            mode='area', \n",
    "            recompute_scale_factor=True\n",
    "        )\n",
    "    return F.grid_sample(batch, coords, mode='bilinear', padding_mode='reflection', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ca54b-1103-4b90-8995-7dcc9ac56d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai augmentations\n",
    "tt_fastai = fastai_aug.ToTensor()\n",
    "i2f_fastai = fastai_aug.IntToFloatTensor()\n",
    "rrc_fastai = fastai_aug.RandomResizedCropGPU((224,224))\n",
    "\n",
    "# torchvision augmentations\n",
    "tt_torch = tvtfms.ToTensor()\n",
    "\n",
    "# apply fastai augmentations\n",
    "base_im_fastai = crop_fastai(im_fastai)\n",
    "result_im_fastai = rrc_fastai(\n",
    "    i2f_fastai(\n",
    "        tt_fastai(base_im_fastai).unsqueeze(0)\n",
    "    ), split_idx=1\n",
    ")\n",
    "\n",
    "# apply torchvision augmentations\n",
    "result_im_tv = gpu_crop(tt_torch(tfmd_img).unsqueeze(0), (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9ba76-3333-4320-adc9-68cb20c03287",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(result_im_fastai, result_im_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530430a-12fd-4a68-87b2-48518b943e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_torch = tvtfms.Normalize([0.485, 0.456, 0.406], [0.229,0.224,0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888d3ca-56d5-4c39-882a-b35fb58e3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai augmentations\n",
    "norm_fastai = fastai_aug.Normalize.from_stats(*fastai_aug.imagenet_stats, cuda=False)\n",
    "# apply fastai augmentations\n",
    "base_im_fastai = crop_fastai(im_fastai)\n",
    "result_im_fastai = norm_fastai(\n",
    "    rrc_fastai(\n",
    "        i2f_fastai(\n",
    "            tt_fastai(base_im_fastai).unsqueeze(0)\n",
    "        ), split_idx=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# apply torchvision augmentations\n",
    "result_im_tv = norm_torch(gpu_crop(tt_torch(tfmd_img).unsqueeze(0), (224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deffdc8-c0c3-43d2-9732-2d6e8405b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(result_im_fastai, result_im_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97c353-8e08-4ff8-b66d-e2783ec98b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as tvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7033592-efab-44f9-9e0d-5640f5d4a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image:typing.Union[Image.Image, torch.tensor], size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and crops it `size` unless one \n",
    "    dimension is larger than the actual image. Padding \n",
    "    must be performed afterwards if so.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform cropping on\n",
    "        size (`tuple` of integers):\n",
    "            A size to crop to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    top = max(top, 0)\n",
    "    left = max(left, 0)\n",
    "    \n",
    "    height = min(top + size[0], image.shape[-1])\n",
    "    width = min(left + size[1], image.shape[-2])\n",
    "    return image.crop((top, left, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65146c-6caf-4890-9d3c-9cced4620d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(image, size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and pads it to `size` with\n",
    "    zeros.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform padding on\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    pad_top = max(-top, 0)\n",
    "    pad_left = max(-left, 0)\n",
    "    \n",
    "    height, width = (\n",
    "        max(size[1] - image.shape[-1] + top, 0), \n",
    "        max(size[0] - image.shape[-2] + left, 0)\n",
    "    )\n",
    "    return tvf.pad(\n",
    "        image, \n",
    "        [pad_top, pad_left, height, width], \n",
    "        padding_mode=\"constant\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1406278-0a0e-43e1-b68a-62d8fe54ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_crop(\n",
    "    batch:torch.tensor, \n",
    "    size:typing.Tuple[int,int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Crops each image in `batch` to a particular `size`.\n",
    "    \n",
    "    Args:\n",
    "        batch (array of `torch.Tensor`):\n",
    "            A batch of images, should be of shape `NxCxWxH`\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        A batch of cropped images\n",
    "    \"\"\"\n",
    "   \n",
    "    affine_matrix = torch.eye(3, device=batch.device).float()\n",
    "    affine_matrix = affine_matrix.unsqueeze(0)\n",
    "    affine_matrix = affine_matrix.expand(batch.size(0), 3, 3)\n",
    "    affine_matrix = affine_matrix.contiguous()[:,:2]\n",
    "    \n",
    "    coords = F.affine_grid(\n",
    "        affine_matrix, batch.shape[:2] + size, align_corners=True\n",
    "    )\n",
    "    \n",
    "    top_range, bottom_range = coords.min(), coords.max()\n",
    "    zoom = 1/(bottom_range - top_range).item()*2\n",
    "    \n",
    "    resizing_limit = min(\n",
    "        batch.shape[-2]/coords.shape[-2],\n",
    "        batch.shape[-1]/coords.shape[-1]\n",
    "    )/2\n",
    "    \n",
    "    if resizing_limit > 1 and resizing_limit > zoom:\n",
    "        batch = F.interpolate(\n",
    "            batch, \n",
    "            scale_factor=1/resizing_limit, \n",
    "            mode='area', \n",
    "            recompute_scale_factor=True\n",
    "        )\n",
    "    return F.grid_sample(batch, coords, mode='bilinear', padding_mode='reflection', align_corners=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dbeaabf96d056229716848a298cd9413f5c098c5e85ebec7037464305d96e83e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
