{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "396a4e1c-a5b6-4093-aac3-b42f305a613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tvf\n",
    "import torchvision.transforms as tvtfms\n",
    "import operator as op\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "# For type hinting later on\n",
    "import collections\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf18e089-4375-4aa3-8f62-fac603f458f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_model(\"vit_tiny_patch16_224\", pretrained=False, num_classes=0, in_chans=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55b7142-69e6-4182-85f6-893a51dee085",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = nn.Sequential(\n",
    "    nn.BatchNorm1d(192),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(192, 512, bias=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 37, bias=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c8c22b-eca7-40d4-97bd-fc28421dad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(net, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6868944-1498-422e-994e-6fa15fd2136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"models/MyModel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbd956-0d28-45ee-aa5d-33eff1a32b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba897ed8-1df7-4240-834c-a39d23ddb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b5f1e0-5605-49ff-8405-d929f102ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(state.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5099e3-8379-4946-85a3-aeb672489e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weight(name, parameter, state_dict):\n",
    "    \"\"\"\n",
    "    Takes in a layer `name`, model `parameter`, and `state_dict`\n",
    "    and loads the weights from `state_dict` into `parameter`\n",
    "    if it exists.\n",
    "    \"\"\"\n",
    "   \n",
    "    if name[0] == \"0\":\n",
    "        name = name[:2] + \"model.\" + name[2:]\n",
    "    if name in state_dict.keys():\n",
    "        input_parameter = state_dict[name]\n",
    "        if input_parameter.shape == parameter.shape:\n",
    "            parameter.copy_(input_parameter)\n",
    "        else:\n",
    "            print(f'Shape mismatch at layer: {name}, skipping')\n",
    "    else:\n",
    "        print(f'{name} is not in the state_dict, skipping.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ec54c5-4d27-43ac-aaf0-00eae7b18fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weights(input_model:nn.Module, input_weights:collections.OrderedDict, application_function:callable):\n",
    "    \"\"\"\n",
    "    Takes an input state_dict and applies those weights to the `input_model`, potentially \n",
    "    with a modifier function.\n",
    "    \n",
    "    Args:\n",
    "        input_model (`nn.Module`):\n",
    "            The model that weights should be applied to\n",
    "        input_weights (`collections.OrderedDict`):\n",
    "            A dictionary of weights, the trained model's `state_dict()`\n",
    "        application_function (`callable`):\n",
    "            A function that takes in one parameter and layer name from `input_model`\n",
    "            and the `input_weights`. Should apply the weights from the state dict into `input_model`.\n",
    "    \"\"\"\n",
    "    model_dict = input_model.state_dict()\n",
    "    for name, parameter in model_dict.items():\n",
    "        application_function(name, parameter, input_weights)\n",
    "    input_model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c884993-cb52-412e-9cad-8d1b9dfff70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_weights(model, state, copy_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e5620c43-fb68-42e1-becd-96c74046a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.data import PILImage\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import get_image_files\n",
    "import fastai.vision.augment as fastai_aug\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0ea83f-8b9b-45c1-baf8-ee43adde1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'\n",
    "fname = get_image_files(path)[0]\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31184db2-06a4-46da-beba-864345c6dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pil = Image.open(fname)\n",
    "im_fastai = PILImage.create(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "603843e3-5448-47c9-a937-87e0a3bfe886",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.array(im_pil) == np.array(im_fastai)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebe49581-c4e6-4d73-8b74-e7875962f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_fastai = fastai_aug.CropPad((460,460))\n",
    "crop_torch = tvtfms.CenterCrop((460,460))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "522ae350-afb6-41f0-bad3-1ddbf52bb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.array(crop_fastai(im_fastai)) == np.array(crop_torch(im_pil))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2e2ce70b-d412-47b3-8f8b-d9c1af4c02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image:typing.Union[Image.Image, torch.tensor], size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and crops it `size` unless one \n",
    "    dimension is larger than the actual image. Padding \n",
    "    must be performed afterwards if so.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform cropping on\n",
    "        size (`tuple` of integers):\n",
    "            A size to crop to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    top = max(top, 0)\n",
    "    left = max(left, 0)\n",
    "    \n",
    "    height = min(top + size[0], image.shape[-1])\n",
    "    width = min(left + size[1], image.shape[-2])\n",
    "    return image.crop((top, left, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e55abb74-5e23-4930-a9f3-b260e6b99aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(image, size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and pads it to `size` with\n",
    "    zeros.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform padding on\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    pad_top = max(-top, 0)\n",
    "    pad_left = max(-left, 0)\n",
    "    \n",
    "    height, width = (\n",
    "        max(size[1] - image.shape[-1] + top, 0), \n",
    "        max(size[0] - image.shape[-2] + left, 0)\n",
    "    )\n",
    "    return tvf.pad(\n",
    "        image, \n",
    "        [pad_top, pad_left, height, width], \n",
    "        padding_mode=\"constant\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "35f08caa-723e-4167-8756-ec79b9a7698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (460,460)\n",
    "tfmd_img = pad(crop(im_pil, size),size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0d384de6-bb93-4296-a664-a5d5a530dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(tfmd_img) == crop_fastai(im_fastai)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "32b23700-4b94-4775-b5f9-cee632f39b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_crop(\n",
    "    batch:torch.tensor, \n",
    "    size:typing.Tuple[int,int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Crops each image in `batch` to a particular `size`.\n",
    "    \n",
    "    Args:\n",
    "        batch (array of `torch.Tensor`):\n",
    "            A batch of images, should be of shape `NxCxWxH`\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        A batch of cropped images\n",
    "    \"\"\"\n",
    "   \n",
    "    affine_matrix = torch.eye(3, device=batch.device).float()\n",
    "    affine_matrix = affine_matrix.unsqueeze(0)\n",
    "    affine_matrix = affine_matrix.expand(batch.size(0), 3, 3)\n",
    "    affine_matrix = affine_matrix.contiguous()[:,:2]\n",
    "    \n",
    "    coords = F.affine_grid(\n",
    "        affine_matrix, batch.shape[:2] + size, align_corners=True\n",
    "    )\n",
    "    \n",
    "    top_range, bottom_range = coords.min(), coords.max()\n",
    "    zoom = 1/(bottom_range - top_range).item()*2\n",
    "    \n",
    "    resizing_limit = min(\n",
    "        batch.shape[-2]/coords.shape[-2],\n",
    "        batch.shape[-1]/coords.shape[-1]\n",
    "    )/2\n",
    "    \n",
    "    if resizing_limit > 1 and resizing_limit > zoom:\n",
    "        batch = F.interpolate(\n",
    "            batch, \n",
    "            scale_factor=1/resizing_limit, \n",
    "            mode='area', \n",
    "            recompute_scale_factor=True\n",
    "        )\n",
    "    return F.grid_sample(batch, coords, mode='bilinear', padding_mode='reflection', align_corners=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "eb7ca54b-1103-4b90-8995-7dcc9ac56d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai augmentations\n",
    "tt_fastai = fastai_aug.ToTensor()\n",
    "i2f_fastai = fastai_aug.IntToFloatTensor()\n",
    "rrc_fastai = fastai_aug.RandomResizedCropGPU((224,224))\n",
    "\n",
    "# torchvision augmentations\n",
    "tt_torch = tvtfms.ToTensor()\n",
    "\n",
    "# apply fastai augmentations\n",
    "base_im_fastai = crop_fastai(im_fastai)\n",
    "result_im_fastai = rrc_fastai(\n",
    "    i2f_fastai(\n",
    "        tt_fastai(base_im_fastai).unsqueeze(0)\n",
    "    ), split_idx=1\n",
    ")\n",
    "\n",
    "# apply torchvision augmentations\n",
    "result_im_tv = gpu_crop(tt_torch(tfmd_img).unsqueeze(0), (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4cb9ba76-3333-4320-adc9-68cb20c03287",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(result_im_fastai, result_im_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a530430a-12fd-4a68-87b2-48518b943e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_torch = tvtfms.Normalize([0.485, 0.456, 0.406], [0.229,0.224,0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1888d3ca-56d5-4c39-882a-b35fb58e3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai augmentations\n",
    "norm_fastai = fastai_aug.Normalize.from_stats(*fastai_aug.imagenet_stats, cuda=False)\n",
    "# apply fastai augmentations\n",
    "base_im_fastai = crop_fastai(im_fastai)\n",
    "result_im_fastai = norm_fastai(\n",
    "    rrc_fastai(\n",
    "        i2f_fastai(\n",
    "            tt_fastai(base_im_fastai).unsqueeze(0)\n",
    "        ), split_idx=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# apply torchvision augmentations\n",
    "result_im_tv = norm_torch(gpu_crop(tt_torch(tfmd_img).unsqueeze(0), (224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4deffdc8-c0c3-43d2-9732-2d6e8405b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(result_im_fastai, result_im_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97c353-8e08-4ff8-b66d-e2783ec98b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as tvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7033592-efab-44f9-9e0d-5640f5d4a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image:typing.Union[Image.Image, torch.tensor], size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and crops it `size` unless one \n",
    "    dimension is larger than the actual image. Padding \n",
    "    must be performed afterwards if so.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform cropping on\n",
    "        size (`tuple` of integers):\n",
    "            A size to crop to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    top = max(top, 0)\n",
    "    left = max(left, 0)\n",
    "    \n",
    "    height = min(top + size[0], image.shape[-1])\n",
    "    width = min(left + size[1], image.shape[-2])\n",
    "    return image.crop((top, left, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65146c-6caf-4890-9d3c-9cced4620d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(image, size:typing.Tuple[int,int]) -> Image:\n",
    "    \"\"\"\n",
    "    Takes a `PIL.Image` and pads it to `size` with\n",
    "    zeros.\n",
    "    \n",
    "    Args:\n",
    "        image (`PIL.Image`):\n",
    "            An image to perform padding on\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        An augmented `PIL.Image`\n",
    "    \"\"\"\n",
    "    top = (image.shape[-1] - size[0]) // 2\n",
    "    left = (image.shape[-2] - size[1]) // 2\n",
    "    \n",
    "    pad_top = max(-top, 0)\n",
    "    pad_left = max(-left, 0)\n",
    "    \n",
    "    height, width = (\n",
    "        max(size[1] - image.shape[-1] + top, 0), \n",
    "        max(size[0] - image.shape[-2] + left, 0)\n",
    "    )\n",
    "    return tvf.pad(\n",
    "        image, \n",
    "        [pad_top, pad_left, height, width], \n",
    "        padding_mode=\"constant\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1406278-0a0e-43e1-b68a-62d8fe54ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_crop(\n",
    "    batch:torch.tensor, \n",
    "    size:typing.Tuple[int,int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Crops each image in `batch` to a particular `size`.\n",
    "    \n",
    "    Args:\n",
    "        batch (array of `torch.Tensor`):\n",
    "            A batch of images, should be of shape `NxCxWxH`\n",
    "        size (`tuple` of integers):\n",
    "            A size to pad to, should be in the form\n",
    "            of (width, height)\n",
    "            \n",
    "    Returns:\n",
    "        A batch of cropped images\n",
    "    \"\"\"\n",
    "   \n",
    "    affine_matrix = torch.eye(3, device=batch.device).float()\n",
    "    affine_matrix = affine_matrix.unsqueeze(0)\n",
    "    affine_matrix = affine_matrix.expand(batch.size(0), 3, 3)\n",
    "    affine_matrix = affine_matrix.contiguous()[:,:2]\n",
    "    \n",
    "    coords = F.affine_grid(\n",
    "        affine_matrix, batch.shape[:2] + size, align_corners=True\n",
    "    )\n",
    "    \n",
    "    top_range, bottom_range = coords.min(), coords.max()\n",
    "    zoom = 1/(bottom_range - top_range).item()*2\n",
    "    \n",
    "    resizing_limit = min(\n",
    "        batch.shape[-2]/coords.shape[-2],\n",
    "        batch.shape[-1]/coords.shape[-1]\n",
    "    )/2\n",
    "    \n",
    "    if resizing_limit > 1 and resizing_limit > zoom:\n",
    "        batch = F.interpolate(\n",
    "            batch, \n",
    "            scale_factor=1/resizing_limit, \n",
    "            mode='area', \n",
    "            recompute_scale_factor=True\n",
    "        )\n",
    "    return F.grid_sample(batch, coords, mode='bilinear', padding_mode='reflection', align_corners=True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
